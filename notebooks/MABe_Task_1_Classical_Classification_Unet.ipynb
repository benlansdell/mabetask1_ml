{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4w4yZOfV5le"
   },
   "source": [
    "<h1> <center>\n",
    "ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸ<br>\n",
    "ğŸ€ MABe Classical Classification: UNet ğŸ’ª ğŸ<br>\n",
    "ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸ€ğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸ\n",
    "</center>\n",
    "</h1>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://images.aicrowd.com/uploads/ckeditor/pictures/324/content_task1_structure.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsaWsU8dILfN"
   },
   "source": [
    "# Import necessary modules and packages ğŸ“š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G4CVVoCjIN95"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "import keras.layers as layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "import tqdm\n",
    "import gc\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout, Reshape, Conv1D, BatchNormalization, Activation, AveragePooling1D, GlobalAveragePooling1D, Lambda, Input, Concatenate, Add, UpSampling1D, Multiply\n",
    "from keras.models import Model\n",
    "from keras.objectives import mean_squared_error\n",
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\n",
    "from keras.initializers import random_normal\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekJZdhqrvTRa",
    "outputId": "95eb3ca8-9d92-4d1b-a2c7-77048d726ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAPI Key valid\u001b[0m\r\n",
      "\u001b[32mSaved API Key successfully!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"0ba231d61506b40a4ae00df011cf0cb9\"\n",
    "!aicrowd login --api-key $API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/blansdel/projects/mabe-task1\n"
     ]
    }
   ],
   "source": [
    "cd /home/blansdel/projects/mabe-task1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md4em-KzIofe"
   },
   "source": [
    "Extract the downloaded dataset to `data` directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MLxouZUvTRa"
   },
   "source": [
    "# Load Data\n",
    "The dataset files are python dictionaries, [this](https://colab.research.google.com/drive/1ddCX-TAdEcsUaGf09f5Glgr_G57FMK_O#scrollTo=JPsfxdl2GMcM&line=18&uniqifier=1) is a descirption of how the data is organized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RopVoFl1vTRb"
   },
   "outputs": [],
   "source": [
    "train = np.load('data/train.npy',allow_pickle=True).item()\n",
    "test = np.load('data/test.npy',allow_pickle=True).item()\n",
    "sample_submission = np.load('data/sample_submission.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iKG5MfYIu1B"
   },
   "source": [
    "## Seeding helper\n",
    "Its good practice to seed before every run, that way its easily reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r4rNTUuFaHmo"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "  np.random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "  tf.random.set_seed(seed)\n",
    "\n",
    "seed=2021\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cbXzIDFaA5o"
   },
   "source": [
    "## Generator ğŸ”Œ\n",
    "\n",
    "The generator is used to take input winodws from each sequence after randomly sampling frames. \n",
    "\n",
    "It also provides code for augmentations\n",
    "1.   Random rotation\n",
    "2.   Random translate\n",
    "\n",
    "ğŸš§ Note that these augmentations are applied in the same across all frames in a selected window, e.g - Random rotation by 10 degrees will rotate all frames in the input window by the same angle.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lAPk-wgqaETQ"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Check batch size/dimensions are right here for the Unet\n",
    "\n",
    "class MABe_Generator(keras.utils.Sequence):\n",
    "    def __init__(self, pose_dict, \n",
    "                 batch_size, dim, \n",
    "                 use_conv, num_classes, augment=False,\n",
    "                 class_to_number=None,\n",
    "                 past_frames=0, future_frames=0, \n",
    "                 frame_gap=1, shuffle=False,\n",
    "                 mode='fit'):\n",
    "        self.batch_size = batch_size\n",
    "        self.video_keys = list(pose_dict.keys())\n",
    "        self.dim = dim\n",
    "        self.use_conv = use_conv\n",
    "        self.past_frames = past_frames\n",
    "        self.future_frames = future_frames\n",
    "        self.frame_gap = frame_gap\n",
    "        self.shuffle = shuffle\n",
    "        self.num_classes=num_classes\n",
    "        self.augment = augment\n",
    "        self.mode = mode\n",
    "\n",
    "        self.class_to_number = class_to_number\n",
    "\n",
    "        self.video_indexes = []\n",
    "        self.frame_indexes = []\n",
    "        self.X = {}\n",
    "        if self.mode == 'fit':\n",
    "          self.y = []\n",
    "        self.pad = self.past_frames * self.frame_gap\n",
    "        future_pad = self.future_frames * self.frame_gap\n",
    "        pad_width = (self.pad, future_pad), (0, 0), (0, 0), (0, 0)\n",
    "        self.seq_lengths = {}\n",
    "        for vc, key in enumerate(self.video_keys):\n",
    "          if self.mode == 'fit':\n",
    "            anno = pose_dict[key]['annotations']\n",
    "            self.y.extend(anno)\n",
    "          nframes = len(pose_dict[key]['keypoints'])\n",
    "          self.video_indexes.extend([vc for _ in range(nframes)])\n",
    "          self.frame_indexes.extend(range(nframes))\n",
    "          self.X[key] = np.pad(pose_dict[key]['keypoints'], pad_width)\n",
    "          self.seq_lengths[key] = nframes\n",
    "        \n",
    "        if self.mode == 'fit':\n",
    "          self.y = np.array(self.y)\n",
    "        \n",
    "        self.X_dtype = self.X[key].dtype\n",
    "\n",
    "        self.indexes = list(range(len(self.frame_indexes)))\n",
    "\n",
    "        if self.mode == 'predict':\n",
    "          extra_predicts = -len(self.indexes) % self.batch_size # So that last part is not missed\n",
    "          self.indexes.extend(self.indexes[:extra_predicts])\n",
    "          self.indexes = np.array(self.indexes)\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def augment_fn(self, x):\n",
    "      # Rotate\n",
    "      angle = (np.random.rand()-0.5) * (np.pi * 2)\n",
    "      c, s = np.cos(angle), np.sin(angle)\n",
    "      rot = np.array([[c, -s], [s, c]])\n",
    "      x = np.dot(x, rot)\n",
    "\n",
    "      # Shift - All get shifted together\n",
    "      shift = (np.random.rand(2)-0.5) * 2 * 0.25\n",
    "      x = x + shift\n",
    "      return x\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bs = self.batch_size\n",
    "        indexes = self.indexes[index*bs:(index+1)*bs]\n",
    "        X = np.empty((bs, *self.dim), self.X_dtype)\n",
    "        if self.mode == 'predict':\n",
    "          vkey_fi_list = []\n",
    "        for bi, idx in enumerate(indexes):\n",
    "          vkey = self.video_keys[self.video_indexes[idx]]\n",
    "          fi = self.frame_indexes[idx]\n",
    "          if self.mode == 'predict':\n",
    "            vkey_fi_list.append((vkey, fi))\n",
    "          fi = fi + self.pad\n",
    "          start = fi - self.past_frames*self.frame_gap\n",
    "          stop = fi + (self.future_frames + 1)*self.frame_gap\n",
    "          assert start >= 0\n",
    "\n",
    "          Xi = self.X[vkey][start:stop:self.frame_gap].copy()\n",
    "          \n",
    "          if self.augment:\n",
    "            Xi = self.augment_fn(Xi)\n",
    "          X[bi] = np.reshape(Xi, self.dim)\n",
    "          \n",
    "\n",
    "        if self.mode == 'fit':\n",
    "          y_vals = self.y[indexes]\n",
    "          # Converting to one hot because F1 callback needs one hot\n",
    "          y = np.zeros( (bs,self.num_classes), np.float32)\n",
    "          y[np.arange(bs), y_vals] = 1\n",
    "          return X, y\n",
    "\n",
    "        elif self.mode == 'predict':\n",
    "          return X, vkey_fi_list\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_WfaWn9JCWX"
   },
   "source": [
    "## Trainer ğŸ‹ï¸\n",
    "\n",
    "The trainer class implements a unified interface for using the datagenerator.\n",
    "\n",
    "It supports fully connected or 1D convolutional networks, as well as other hyperparameters for the model and the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NYw6y0peZ-ch"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Build UNet instead of 1D CNN...\n",
    "\n",
    "class Trainer:\n",
    "  def __init__(self, *,\n",
    "               train_data,\n",
    "               val_data,\n",
    "               test_data,\n",
    "               feature_dim, \n",
    "               batch_size, \n",
    "               num_classes,\n",
    "               augment=False,\n",
    "               class_to_number=None,\n",
    "               past_frames=0, \n",
    "               future_frames=0,\n",
    "               frame_gap=1, \n",
    "               use_conv=False):\n",
    "    flat_dim = np.prod(feature_dim)\n",
    "    if use_conv:\n",
    "      input_dim = ((past_frames + future_frames + 1), flat_dim,)\n",
    "    else:\n",
    "      input_dim = (flat_dim * (past_frames + future_frames + 1),)\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.use_conv=use_conv\n",
    "    self.num_classes=num_classes\n",
    "\n",
    "    c2n = {'other': 0,'investigation': 1,\n",
    "                'attack' : 2, 'mount' : 3}\n",
    "    self.class_to_number = class_to_number or c2n\n",
    "\n",
    "    self.train_generator = MABe_Generator(train_data, \n",
    "                                      batch_size=batch_size, \n",
    "                                      dim=input_dim,\n",
    "                                      num_classes=num_classes, \n",
    "                                      past_frames=past_frames, \n",
    "                                      future_frames=future_frames,\n",
    "                                      class_to_number=self.class_to_number,\n",
    "                                      use_conv=use_conv,\n",
    "                                      frame_gap=frame_gap,\n",
    "                                      augment=augment,\n",
    "                                      shuffle=True,\n",
    "                                      mode='fit')\n",
    "\n",
    "    self.val_generator = MABe_Generator(val_data, \n",
    "                                        batch_size=batch_size, \n",
    "                                        dim=input_dim, \n",
    "                                        num_classes=num_classes, \n",
    "                                        past_frames=past_frames,\n",
    "                                        future_frames=future_frames,\n",
    "                                        use_conv=use_conv,\n",
    "                                        class_to_number=self.class_to_number,\n",
    "                                        frame_gap=frame_gap,\n",
    "                                        augment=False,\n",
    "                                        shuffle=False,\n",
    "                                        mode='fit')\n",
    "    \n",
    "    if test_data is not None:\n",
    "      self.test_generator = MABe_Generator(test_data, \n",
    "                                        batch_size=8192, \n",
    "                                        dim=input_dim, \n",
    "                                        num_classes=num_classes, \n",
    "                                        past_frames=past_frames,\n",
    "                                        future_frames=future_frames,\n",
    "                                        use_conv=use_conv,\n",
    "                                        class_to_number=self.class_to_number,\n",
    "                                        frame_gap=frame_gap,\n",
    "                                        augment=False,\n",
    "                                        shuffle=False,\n",
    "                                        mode='predict')\n",
    "  \n",
    "  def delete_model(self):\n",
    "    self.model = None\n",
    "  \n",
    "  def initialize_model(self, layer_channels=(512, 256), dropout_rate=0., \n",
    "                       learning_rate=1e-3, conv_size=5):\n",
    "\n",
    "    #############################\n",
    "    #Here is the architecture...#\n",
    "    #############################    \n",
    "    \n",
    "    def cbr(x, out_layer, kernel, stride, dilation):\n",
    "        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def se_block(x_in, layer_n):\n",
    "        x = GlobalAveragePooling1D()(x_in)\n",
    "        x = Dense(layer_n//8, activation=\"relu\")(x)\n",
    "        x = Dense(layer_n, activation=\"sigmoid\")(x)\n",
    "        x_out=Multiply()([x_in, x])\n",
    "        return x_out\n",
    "\n",
    "    def resblock(x_in, layer_n, kernel, dilation, use_se=True):\n",
    "        x = cbr(x_in, layer_n, kernel, 1, dilation)\n",
    "        x = cbr(x, layer_n, kernel, 1, dilation)\n",
    "        if use_se:\n",
    "            x = se_block(x, layer_n)\n",
    "        x = Add()([x_in, x])\n",
    "        return x  \n",
    "\n",
    "    def Unet(input_shape=(None,1)):\n",
    "        layer_n = 64\n",
    "        kernel_size = 7\n",
    "        depth = 2\n",
    "\n",
    "        input_layer = Input(input_shape)    \n",
    "        input_layer_1 = AveragePooling1D(5)(input_layer)\n",
    "        input_layer_2 = AveragePooling1D(25)(input_layer)\n",
    "\n",
    "        ########## Encoder\n",
    "        x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n",
    "        for i in range(depth):\n",
    "            x = resblock(x, layer_n, kernel_size, 1)\n",
    "        out_0 = x\n",
    "\n",
    "        x = cbr(x, layer_n*2, kernel_size, 5, 1)\n",
    "        for i in range(depth):\n",
    "            x = resblock(x, layer_n*2, kernel_size, 1)\n",
    "        out_1 = x\n",
    "\n",
    "        x = Concatenate()([x, input_layer_1])    \n",
    "        x = cbr(x, layer_n*3, kernel_size, 5, 1)\n",
    "        for i in range(depth):\n",
    "            x = resblock(x, layer_n*3, kernel_size, 1)\n",
    "        out_2 = x\n",
    "\n",
    "        x = Concatenate()([x, input_layer_2])    \n",
    "        x = cbr(x, layer_n*4, kernel_size, 5, 1)\n",
    "        for i in range(depth):\n",
    "            x = resblock(x, layer_n*4, kernel_size, 1)\n",
    "\n",
    "        ########### Decoder\n",
    "        x = UpSampling1D(5)(x)\n",
    "        x = Concatenate()([x, out_2])\n",
    "        x = cbr(x, layer_n*3, kernel_size, 1, 1)\n",
    "\n",
    "        x = UpSampling1D(5)(x)\n",
    "        x = Concatenate()([x, out_1])\n",
    "        x = cbr(x, layer_n*2, kernel_size, 1, 1)\n",
    "\n",
    "        x = UpSampling1D(5)(x)\n",
    "        x = Concatenate()([x, out_0])\n",
    "        x = cbr(x, layer_n, kernel_size, 1, 1)    \n",
    "\n",
    "        #regressor\n",
    "        #x = Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n",
    "        #out = Activation(\"sigmoid\")(x)\n",
    "        #out = Lambda(lambda x: 12*x)(out)\n",
    "\n",
    "        #classifier\n",
    "        x = Conv1D(11, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n",
    "        out = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = Model(input_layer, out)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def add_dense_bn_activate(model, out_dim, activation='relu', drop=0.):\n",
    "      model.add(layers.Dense(out_dim))\n",
    "      model.add(layers.BatchNormalization())\n",
    "      model.add(layers.Activation('relu'))\n",
    "      if drop > 0:\n",
    "        model.add(layers.Dropout(rate=drop))\n",
    "      return model\n",
    "    \n",
    "    def add_conv_bn_activate(model, out_dim, activation='relu', conv_size=3, drop=0.):\n",
    "      model.add(layers.Conv1D(out_dim, conv_size))\n",
    "      model.add(layers.BatchNormalization())\n",
    "      model.add(layers.Activation('relu'))\n",
    "      model.add(layers.MaxPooling1D(2, 2))\n",
    "      if drop > 0:\n",
    "        model.add(layers.Dropout(rate=drop))\n",
    "      return model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(self.input_dim))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    for ch in layer_channels:\n",
    "      if self.use_conv:\n",
    "        model = add_conv_bn_activate(model, ch, conv_size=conv_size,\n",
    "                                     drop=dropout_rate)\n",
    "      else:\n",
    "        model = add_dense_bn_activate(model, ch, drop=dropout_rate)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "    metrics = [tfa.metrics.F1Score(num_classes=self.num_classes)]\n",
    "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=metrics)\n",
    "\n",
    "    self.model = model\n",
    "\n",
    "  def _set_model(self, model):\n",
    "      \"\"\" Set an external, provide initialized and compiled keras model \"\"\"\n",
    "      self.model = model\n",
    "\n",
    "  def train(self, epochs=20, class_weight=None):\n",
    "    if self.model is None:\n",
    "      print(\"Please Call trainer.initialize_model first\")\n",
    "      return\n",
    "    self.model.fit(self.train_generator,\n",
    "          validation_data=self.val_generator,\n",
    "          epochs=epochs,\n",
    "          class_weight=class_weight)\n",
    "        \n",
    "  def get_validation_labels(self, on_test_set=False):\n",
    "    y_val = []\n",
    "    for _, y in self.val_generator:\n",
    "      y_val.extend(list(y))\n",
    "    y_val = np.argmax(np.array(y_val), axis=-1)\n",
    "    return y_val\n",
    "\n",
    "  def get_validation_predictions(self):\n",
    "    y_val_pred = self.model.predict(self.val_generator)\n",
    "    y_val_pred = np.argmax(y_val_pred, axis=-1)\n",
    "    return y_val_pred\n",
    "\n",
    "  def get_validation_metrics(self):\n",
    "    y_val = self.get_validation_labels()\n",
    "    y_val_pred = self.get_validation_predictions()\n",
    "\n",
    "    f1_scores = sklearn.metrics.f1_score(y_val, y_val_pred,average=None)\n",
    "    rec_scores = sklearn.metrics.precision_score(y_val, y_val_pred,average=None)\n",
    "    prec_scores = sklearn.metrics.recall_score(y_val, y_val_pred,average=None)\n",
    "    classes = list(self.class_to_number.keys())\n",
    "    metrics = pd.DataFrame({\"Class\": classes, \"F1\": f1_scores, \"Precision\": prec_scores, \"Recall\": rec_scores})\n",
    "    return metrics\n",
    "  \n",
    "  def get_test_predictions(self):\n",
    "    all_test_preds = {}\n",
    "    for vkey in self.test_generator.video_keys:\n",
    "      nframes = self.test_generator.seq_lengths[vkey]\n",
    "      all_test_preds[vkey] = np.zeros(nframes, dtype=np.int32)\n",
    "\n",
    "    for X, vkey_fi_list in tqdm.tqdm(self.test_generator):\n",
    "      test_pred = self.model.predict(X)\n",
    "      test_pred = np.argmax(test_pred, axis=-1)\n",
    "\n",
    "      for p, (vkey, fi) in zip(test_pred, vkey_fi_list):\n",
    "        all_test_preds[vkey][fi] = p\n",
    "    return all_test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYYrvXNTaVfU"
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "We'll normalize the data based on the information that the frame size is 1024x570\n",
    "\n",
    "The original data is of shape (sequence length, mouse, x y coordinate, keypoint)\n",
    " = (length, 2, 2, 7)\n",
    "\n",
    " We'll swap the x y and the keypoint axis, which will help in rotation augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xYKbWFVcaSeW"
   },
   "outputs": [],
   "source": [
    "def normalize_data(orig_pose_dictionary):\n",
    "  for key in orig_pose_dictionary:\n",
    "    X = orig_pose_dictionary[key]['keypoints']\n",
    "    X = X.transpose((0,1,3,2)) #last axis is x, y coordinates\n",
    "    X[..., 0] = X[..., 0]/1024\n",
    "    X[..., 1] = X[..., 1]/570\n",
    "    orig_pose_dictionary[key]['keypoints'] = X\n",
    "  return orig_pose_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKOjPhZRL4gk"
   },
   "source": [
    "## Dataset split\n",
    "Since MABe has multiple sequences, it is sensible to split the dataset based on different sequences rather than randomly sampling frames, which may leak information.\n",
    "\n",
    "About half the sequences don't have \"attack\" behavior, hence we'll stratify based on whether \"attack\" behavior is present or absent.\n",
    "\n",
    "This function only does a single split, but you can also do multiple splits for cross validation.\n",
    "\n",
    "For Task 2 and 3 there are very few sequences, hence we split the sequences in half for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "C7iuMRKKakls"
   },
   "outputs": [],
   "source": [
    "def split_validation(orig_pose_dictionary, vocabulary, seed=2021, \n",
    "                       test_size=0.5, split_videos=False):\n",
    "  if split_videos:\n",
    "    pose_dictionary = {}\n",
    "    for key in orig_pose_dictionary:\n",
    "      key_pt1 = key + '_part1'\n",
    "      key_pt2 = key + '_part2'\n",
    "      anno_len = len(orig_pose_dictionary[key]['annotations'])\n",
    "      split_idx = anno_len//2\n",
    "      pose_dictionary[key_pt1] = {\n",
    "          'annotations': orig_pose_dictionary[key]['annotations'][:split_idx],\n",
    "          'keypoints': orig_pose_dictionary[key]['keypoints'][:split_idx]}\n",
    "      pose_dictionary[key_pt2] = {\n",
    "          'annotations': orig_pose_dictionary[key]['annotations'][split_idx:],\n",
    "          'keypoints': orig_pose_dictionary[key]['keypoints'][split_idx:]}\n",
    "  else:\n",
    "    pose_dictionary = orig_pose_dictionary\n",
    "  \n",
    "  def get_percentage(sequence_key):\n",
    "    anno_seq = num_to_text(pose_dictionary[sequence_key]['annotations'])\n",
    "    counts = {k: np.mean(np.array(anno_seq) == k) for k in vocabulary}\n",
    "    return counts\n",
    "\n",
    "  anno_percentages = {k: get_percentage(k) for k in pose_dictionary}\n",
    "\n",
    "  anno_perc_df = pd.DataFrame(anno_percentages).T\n",
    "\n",
    "  rng_state = np.random.RandomState(seed)\n",
    "  try:\n",
    "    idx_train, idx_val = train_test_split(anno_perc_df.index,\n",
    "                                      stratify=anno_perc_df['attack'] > 0, \n",
    "                                      test_size=test_size,\n",
    "                                      random_state=rng_state)\n",
    "  except:\n",
    "    idx_train, idx_val = train_test_split(anno_perc_df.index,\n",
    "                                      test_size=test_size,\n",
    "                                      random_state=rng_state)\n",
    "    \n",
    "  train_data = {k : pose_dictionary[k] for k in idx_train}\n",
    "  val_data = {k : pose_dictionary[k] for k in idx_val}\n",
    "  return train_data, val_data, anno_perc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neIzSJVvMwyo"
   },
   "source": [
    "# Train function and inference\n",
    "\n",
    "This below function is specific for Task 1, it has a set of hyperparameters we found with some tuning. Though results can be improved with further tuning.\n",
    "\n",
    "It also generates the submission dictionary after training is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cJaqWkXgamKv"
   },
   "outputs": [],
   "source": [
    "def run_task1(results_dir, dataset, vocabulary, test_data, \n",
    "              augment=False, epochs=15, skip_test_prediction=False, seed=2021):\n",
    "  HPARAMS = {}\n",
    "  val_size = HPARAMS[\"val_size\"] = 0.2\n",
    "  normalize = HPARAMS[\"normalize\"] = True\n",
    "  HPARAMS[\"seed\"] = seed\n",
    "  seed_everything(seed)\n",
    "  split_videos = HPARAMS[\"split_videos\"] = False\n",
    "\n",
    "  if normalize:\n",
    "    dataset = normalize_data(deepcopy(dataset))\n",
    "    if not skip_test_prediction:\n",
    "      test_data = normalize_data(deepcopy(test_data))\n",
    "    else:\n",
    "      test_data = None\n",
    "\n",
    "  train_data, val_data, anno_perc_df = split_validation(dataset, \n",
    "                                                        seed=seed,\n",
    "                                                        vocabulary=vocabulary,\n",
    "                                                        test_size=val_size, \n",
    "                                                        split_videos=split_videos)                               \n",
    "  num_classes = len(anno_perc_df.keys())\n",
    "  feature_dim = HPARAMS[\"feature_dim\"] = (2,7,2)\n",
    "\n",
    "  # Generator parameters\n",
    "  past_frames = HPARAMS[\"past_frames\"] = 50\n",
    "  future_frames = HPARAMS[\"future_frames\"] = 50\n",
    "  frame_gap = HPARAMS[\"frame_gap\"] = 1\n",
    "  use_conv = HPARAMS[\"use_conv\"] = True\n",
    "  batch_size = HPARAMS[\"batch_size\"] = 128\n",
    "\n",
    "  # Model parameters\n",
    "  dropout_rate = HPARAMS[\"dropout_rate\"] = 0.5\n",
    "  learning_rate = HPARAMS[\"learning_rate\"] = 5e-4\n",
    "  layer_channels = HPARAMS[\"layer_channels\"] = (128, 64, 32)\n",
    "  conv_size = HPARAMS[\"conv_size\"] = 5\n",
    "  augment = HPARAMS[\"augment\"] = augment\n",
    "  class_to_number = HPARAMS['class_to_number'] = vocabulary\n",
    "  epochs = HPARAMS[\"epochs\"] = epochs\n",
    "\n",
    "  trainer = Trainer(train_data=train_data,\n",
    "                    val_data=val_data,\n",
    "                    test_data=test_data,\n",
    "                    feature_dim=feature_dim, \n",
    "                    batch_size=batch_size, \n",
    "                    num_classes=num_classes,\n",
    "                    augment=augment,\n",
    "                    class_to_number=class_to_number,\n",
    "                    past_frames=past_frames, \n",
    "                    future_frames=future_frames,\n",
    "                    frame_gap=frame_gap,\n",
    "                    use_conv=use_conv)\n",
    "\n",
    "  trainer.initialize_model(layer_channels=layer_channels,\n",
    "                          dropout_rate=dropout_rate,\n",
    "                          learning_rate=learning_rate,\n",
    "                          conv_size=conv_size)\n",
    "  \n",
    "  trainer.train(epochs=epochs)\n",
    "  augment_str = '_augmented' if augment else ''\n",
    "  trainer.model.save(f'{results_dir}/task1{augment_str}.h5')\n",
    "  np.save(f\"{results_dir}/task1{augment_str}_hparams\", HPARAMS)\n",
    "\n",
    "  val_metrics = trainer.get_validation_metrics()\n",
    "  val_metrics.to_csv(f\"{results_dir}/task1_metrics_val.csv\", index=False)\n",
    "\n",
    "  if not skip_test_prediction:\n",
    "    test_results = trainer.get_test_predictions()\n",
    "    np.save(f\"{results_dir}/test_results\", test_results)\n",
    "  else:\n",
    "    test_results = {}\n",
    "\n",
    "  del trainer # clear ram as the test dataset is large\n",
    "  gc.collect()\n",
    "  return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "AnitybveaoQN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3192/3192 [==============================] - 82s 8ms/step - loss: 0.5828 - f1_score: 0.5007 - val_loss: 0.3691 - val_f1_score: 0.6239\n",
      "Epoch 2/15\n",
      "3192/3192 [==============================] - 24s 8ms/step - loss: 0.3415 - f1_score: 0.7544 - val_loss: 0.3264 - val_f1_score: 0.7000\n",
      "Epoch 3/15\n",
      "3192/3192 [==============================] - 24s 8ms/step - loss: 0.3088 - f1_score: 0.8055 - val_loss: 0.2996 - val_f1_score: 0.7077\n",
      "Epoch 4/15\n",
      "3192/3192 [==============================] - 24s 8ms/step - loss: 0.2915 - f1_score: 0.8194 - val_loss: 0.2957 - val_f1_score: 0.7338\n",
      "Epoch 5/15\n",
      "3192/3192 [==============================] - 25s 8ms/step - loss: 0.2798 - f1_score: 0.8292 - val_loss: 0.3085 - val_f1_score: 0.7188\n",
      "Epoch 6/15\n",
      "3192/3192 [==============================] - 26s 8ms/step - loss: 0.2710 - f1_score: 0.8382 - val_loss: 0.2981 - val_f1_score: 0.7309\n",
      "Epoch 7/15\n",
      "3192/3192 [==============================] - 25s 8ms/step - loss: 0.2629 - f1_score: 0.8463 - val_loss: 0.2950 - val_f1_score: 0.7245\n",
      "Epoch 8/15\n",
      "3192/3192 [==============================] - 25s 8ms/step - loss: 0.2563 - f1_score: 0.8512 - val_loss: 0.3068 - val_f1_score: 0.7497\n",
      "Epoch 9/15\n",
      "3192/3192 [==============================] - 25s 8ms/step - loss: 0.2488 - f1_score: 0.8574 - val_loss: 0.2998 - val_f1_score: 0.7481\n",
      "Epoch 10/15\n",
      "3192/3192 [==============================] - 25s 8ms/step - loss: 0.2464 - f1_score: 0.8578 - val_loss: 0.3070 - val_f1_score: 0.7247\n",
      "Epoch 11/15\n",
      "3192/3192 [==============================] - 26s 8ms/step - loss: 0.2380 - f1_score: 0.8630 - val_loss: 0.3016 - val_f1_score: 0.7640\n",
      "Epoch 12/15\n",
      "3192/3192 [==============================] - 26s 8ms/step - loss: 0.2350 - f1_score: 0.8658 - val_loss: 0.2811 - val_f1_score: 0.7515\n",
      "Epoch 13/15\n",
      "3192/3192 [==============================] - 25s 8ms/step - loss: 0.2327 - f1_score: 0.8674 - val_loss: 0.2968 - val_f1_score: 0.7478\n",
      "Epoch 14/15\n",
      "3192/3192 [==============================] - 24s 8ms/step - loss: 0.2303 - f1_score: 0.8689 - val_loss: 0.2928 - val_f1_score: 0.7618\n",
      "Epoch 15/15\n",
      "3192/3192 [==============================] - 25s 8ms/step - loss: 0.2267 - f1_score: 0.8705 - val_loss: 0.3147 - val_f1_score: 0.7519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 998/998 [10:32<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dir = '.'\n",
    "submission = run_task1(results_dir,\n",
    "                       dataset=train['sequences'], \n",
    "                       vocabulary=train['vocabulary'],\n",
    "                       test_data=test['sequences'],\n",
    "                       seed=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZYfbeJIrNt9"
   },
   "source": [
    "# Validate the submission âœ…\n",
    "The submssion should follow these constraints:\n",
    "\n",
    "1.   It should be a dictionary\n",
    "2.   It should be have same keys as sample_submission\n",
    "3.   The lengths of the arrays are same\n",
    "4.   All values are intergers\n",
    "\n",
    "You can use the helper function below to check these\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nl5FrGHcrLy4"
   },
   "outputs": [],
   "source": [
    "def validate_submission(submission, sample_submission):\n",
    "    if not isinstance(submission, dict):\n",
    "      print(\"Submission should be dict\")\n",
    "      return False\n",
    "\n",
    "    if not submission.keys() == sample_submission.keys():\n",
    "      print(\"Submission keys don't match\")\n",
    "      return False\n",
    "    \n",
    "    for key in submission:\n",
    "      sv = submission[key]\n",
    "      ssv = sample_submission[key]\n",
    "      if not len(sv) == len(ssv):\n",
    "        print(f\"Submission lengths of {key} doesn't match\")\n",
    "        return False\n",
    "    \n",
    "    for key, sv in submission.items():\n",
    "      if not all(isinstance(x, (np.int32, np.int64, int)) for x in list(sv)):\n",
    "        print(f\"Submission of {key} is not all integers\")\n",
    "        return False\n",
    "    \n",
    "    print(\"All tests passed\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_twyjQEyaUy",
    "outputId": "9e172484-b8e1-4584-d8a0-d9410e962947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_submission(submission, sample_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPfKXGb1vTRd"
   },
   "source": [
    "## Save the prediction as `npy` ğŸ“¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1LkuPd5AvTRd"
   },
   "outputs": [],
   "source": [
    "np.save(\"submission.npy\", submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEZqmoHJJl4j"
   },
   "source": [
    "## Submit to AIcrowd ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaUu2j8tCdZ1",
    "outputId": "ff0d3f06-544b-443b-e311-d91e370822f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[1;34msubmission.npy\u001b[0m \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100.0%\u001b[0m â€¢ \u001b[32m32.7/32.7 MB\u001b[0m â€¢ \u001b[31m3.3 MB/s\u001b[0m â€¢ \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h                                                                    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®                                                                    \n",
      "                                                                    â”‚ \u001b[1mSuccessfully submitted!\u001b[0m â”‚                                                                    \n",
      "                                                                    â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯                                                                    \n",
      "\u001b[3m                                                                          Important links                                                                          \u001b[0m\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  This submission â”‚\u001b[1;94m \u001b[0m\u001b[1;94mhttps://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications/submissions/125873             \u001b[0m\u001b[1;94m \u001b[0mâ”‚\n",
      "â”‚                  â”‚                                                                                                                                              â”‚\n",
      "â”‚  All submissions â”‚\u001b[1;94m \u001b[0m\u001b[1;94mhttps://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications/submissions?my_submissions=true\u001b[0m\u001b[1;94m \u001b[0mâ”‚\n",
      "â”‚                  â”‚                                                                                                                                              â”‚\n",
      "â”‚      Leaderboard â”‚\u001b[1;94m \u001b[0m\u001b[1;94mhttps://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications/leaderboards                   \u001b[0m\u001b[1;94m \u001b[0mâ”‚\n",
      "â”‚                  â”‚                                                                                                                                              â”‚\n",
      "â”‚ Discussion forum â”‚\u001b[1;94m \u001b[0m\u001b[1;94mhttps://discourse.aicrowd.com/c/multi-agent-behavior-representation-modeling-measurement-and-applications                                   \u001b[0m\u001b[1;94m \u001b[0mâ”‚\n",
      "â”‚                  â”‚                                                                                                                                              â”‚\n",
      "â”‚   Challenge page â”‚\u001b[1;94m \u001b[0m\u001b[1;94mhttps://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications                                \u001b[0m\u001b[1;94m \u001b[0mâ”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "{'submission_id': 125873, 'created_at': '2021-03-11T20:11:38.937Z'}\n"
     ]
    }
   ],
   "source": [
    "!aicrowd submission create -c mabe-task-1-classical-classification -f submission.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVOhbQBK4CdL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of MABe Task 1: Classical Classification Baseline",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
