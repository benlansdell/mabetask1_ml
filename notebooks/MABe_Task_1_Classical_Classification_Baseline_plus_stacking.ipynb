{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4w4yZOfV5le"
   },
   "source": [
    "<h1> <center>\n",
    "🐀🐀🐀🐀🐀🐀🐀🐀🐀🐀🐀🐁🐁🐁🐁🐁🐁🐁🐁🐁🐁<br>\n",
    "🐀 MABe Classical Classification: Baseline 💪 🐁<br>\n",
    "🐀🐀🐀🐀🐀🐀🐀🐀🐀🐀🐀🐁🐁🐁🐁🐁🐁🐁🐁🐁🐁\n",
    "</center>\n",
    "</h1>\n",
    "\n",
    "<p align=\"center\"><img src=\"https://images.aicrowd.com/uploads/ckeditor/pictures/324/content_task1_structure.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select GPU\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/blansdel/projects/mabe-task1\n"
     ]
    }
   ],
   "source": [
    "cd /home/blansdel/projects/mabe-task1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 36731,\n",
      "  \"iopub_port\": 44915,\n",
      "  \"stdin_port\": 40337,\n",
      "  \"control_port\": 34757,\n",
      "  \"hb_port\": 44493,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"d919f831-3b5e62580b9149f8eee826b0\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-c0ed0a48-69c1-4e45-923f-a6d76a0845cf.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "#submission\n",
    "%connect_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsaWsU8dILfN"
   },
   "source": [
    "# Import necessary modules and packages 📚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G4CVVoCjIN95"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "import keras.layers as layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "import tqdm\n",
    "import gc\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split, LeaveOneGroupOut\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score, f1_score, accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from lib.utils import compute_metrics, print_metrics, validate_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwykJ9kzvTRZ"
   },
   "source": [
    "# Download the dataset 📲\n",
    "\n",
    "Please get your API key from https://www.aicrowd.com/participants/me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekJZdhqrvTRa",
    "outputId": "95eb3ca8-9d92-4d1b-a2c7-77048d726ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAPI Key valid\u001b[0m\r\n",
      "\u001b[32mSaved API Key successfully!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"0ba231d61506b40a4ae00df011cf0cb9\"\n",
    "!aicrowd login --api-key $API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MLxouZUvTRa"
   },
   "source": [
    "# Load Data\n",
    "The dataset files are python dictionaries, [this](https://colab.research.google.com/drive/1ddCX-TAdEcsUaGf09f5Glgr_G57FMK_O#scrollTo=JPsfxdl2GMcM&line=18&uniqifier=1) is a descirption of how the data is organized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RopVoFl1vTRb"
   },
   "outputs": [],
   "source": [
    "train = np.load('data/train.npy',allow_pickle=True).item()\n",
    "test = np.load('data/test.npy',allow_pickle=True).item()\n",
    "sample_submission = np.load('data/sample_submission.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCDiH6VCZ_EB"
   },
   "source": [
    "# Training The Model 🏋️‍♂️\n",
    "\n",
    "The given MABe dataset contain many sequences of time series data, each frame has its own behavior label. Training on just a single frame does not give good results due to less information. \n",
    "\n",
    "So here past and future frames are also added to each input. But also all the frames are not concatenated as as the boundaries of the past and future frames need to stay separate for each video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iKG5MfYIu1B"
   },
   "source": [
    "## Seeding helper\n",
    "Its good practice to seed before every run, that way its easily reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "r4rNTUuFaHmo"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "  np.random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "  tf.random.set_seed(seed)\n",
    "\n",
    "seed=2021\n",
    "seed_everything(seed)\n",
    "\n",
    "# Note: Image processing may be slow if too many frames are animated.                \n",
    " \n",
    "#Plotting constants\n",
    "FRAME_WIDTH_TOP = 1024\n",
    "FRAME_HEIGHT_TOP = 570\n",
    " \n",
    "RESIDENT_COLOR = 'lawngreen'\n",
    "INTRUDER_COLOR = 'skyblue'\n",
    " \n",
    "PLOT_MOUSE_START_END = [(0, 1), (0, 2), (1, 3), (2, 3), (3, 4),\n",
    "                        (3, 5), (4, 6), (5, 6), (1, 2)]\n",
    " \n",
    "class_to_color = {'other': 'white', 'attack' : 'red', 'mount' : 'green',\n",
    "                  'investigation': 'orange'}\n",
    " \n",
    "class_to_number = {s: i for i, s in enumerate(train['vocabulary'])}\n",
    " \n",
    "number_to_class = {i: s for i, s in enumerate(train['vocabulary'])}\n",
    " \n",
    "def num_to_text(anno_list):\n",
    "  return np.vectorize(number_to_class.get)(anno_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cbXzIDFaA5o"
   },
   "source": [
    "## Generator 🔌\n",
    "\n",
    "The generator is used to take input winodws from each sequence after randomly sampling frames. \n",
    "\n",
    "It also provides code for augmentations\n",
    "1.   Random rotation\n",
    "2.   Random translate\n",
    "\n",
    "🚧 Note that these augmentations are applied in the same across all frames in a selected window, e.g - Random rotation by 10 degrees will rotate all frames in the input window by the same angle.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lAPk-wgqaETQ"
   },
   "outputs": [],
   "source": [
    "class MABe_Generator(keras.utils.Sequence):\n",
    "    def __init__(self, pose_dict, \n",
    "                 batch_size, dim, \n",
    "                 use_conv, num_classes, augment=False,\n",
    "                 class_to_number=None,\n",
    "                 past_frames=0, future_frames=0, \n",
    "                 frame_gap=1, shuffle=False,\n",
    "                 mode='fit'):\n",
    "        self.batch_size = batch_size\n",
    "        self.video_keys = list(pose_dict.keys())\n",
    "        self.dim = dim\n",
    "        self.use_conv = use_conv\n",
    "        self.past_frames = past_frames\n",
    "        self.future_frames = future_frames\n",
    "        self.frame_gap = frame_gap\n",
    "        self.shuffle = shuffle\n",
    "        self.num_classes=num_classes\n",
    "        self.augment = augment\n",
    "        self.mode = mode\n",
    "\n",
    "        self.class_to_number = class_to_number\n",
    "\n",
    "        self.video_indexes = []\n",
    "        self.frame_indexes = []\n",
    "        self.X = {}\n",
    "        if self.mode == 'fit':\n",
    "          self.y = []\n",
    "        self.pad = self.past_frames * self.frame_gap\n",
    "        future_pad = self.future_frames * self.frame_gap\n",
    "        pad_width = (self.pad, future_pad), (0, 0), (0, 0), (0, 0)\n",
    "        self.seq_lengths = {}\n",
    "        for vc, key in enumerate(self.video_keys):\n",
    "          if self.mode == 'fit':\n",
    "            anno = pose_dict[key]['annotations']\n",
    "            self.y.extend(anno)\n",
    "          nframes = len(pose_dict[key]['keypoints'])\n",
    "          self.video_indexes.extend([vc for _ in range(nframes)])\n",
    "          self.frame_indexes.extend(range(nframes))\n",
    "          self.X[key] = np.pad(pose_dict[key]['keypoints'], pad_width)\n",
    "          self.seq_lengths[key] = nframes\n",
    "        \n",
    "        if self.mode == 'fit':\n",
    "          self.y = np.array(self.y)\n",
    "        \n",
    "        self.X_dtype = self.X[key].dtype\n",
    "\n",
    "        self.indexes = list(range(len(self.frame_indexes)))\n",
    "\n",
    "        if self.mode == 'predict':\n",
    "          extra_predicts = -len(self.indexes) % self.batch_size # So that last part is not missed\n",
    "          self.indexes.extend(self.indexes[:extra_predicts])\n",
    "          self.indexes = np.array(self.indexes)\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def augment_fn(self, x):\n",
    "      # Rotate\n",
    "      angle = (np.random.rand()-0.5) * (np.pi * 2)\n",
    "      c, s = np.cos(angle), np.sin(angle)\n",
    "      rot = np.array([[c, -s], [s, c]])\n",
    "      x = np.dot(x, rot)\n",
    "\n",
    "      # Shift - All get shifted together\n",
    "      shift = (np.random.rand(2)-0.5) * 2 * 0.25\n",
    "      x = x + shift\n",
    "      return x\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bs = self.batch_size\n",
    "        indexes = self.indexes[index*bs:(index+1)*bs]\n",
    "        X = np.empty((bs, *self.dim), self.X_dtype)\n",
    "        if self.mode == 'predict':\n",
    "          vkey_fi_list = []\n",
    "        for bi, idx in enumerate(indexes):\n",
    "          vkey = self.video_keys[self.video_indexes[idx]]\n",
    "          fi = self.frame_indexes[idx]\n",
    "          if self.mode == 'predict':\n",
    "            vkey_fi_list.append((vkey, fi))\n",
    "          fi = fi + self.pad\n",
    "          start = fi - self.past_frames*self.frame_gap\n",
    "          stop = fi + (self.future_frames + 1)*self.frame_gap\n",
    "          assert start >= 0\n",
    "\n",
    "          Xi = self.X[vkey][start:stop:self.frame_gap].copy()\n",
    "          \n",
    "          if self.augment:\n",
    "            Xi = self.augment_fn(Xi)\n",
    "          X[bi] = np.reshape(Xi, self.dim)\n",
    "          \n",
    "\n",
    "        if self.mode == 'fit':\n",
    "          y_vals = self.y[indexes]\n",
    "          # Converting to one hot because F1 callback needs one hot\n",
    "          y = np.zeros( (bs,self.num_classes), np.float32)\n",
    "          y[np.arange(bs), y_vals] = 1\n",
    "          return X, y\n",
    "\n",
    "        elif self.mode == 'predict':\n",
    "          return X, vkey_fi_list\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_WfaWn9JCWX"
   },
   "source": [
    "## Trainer 🏋️\n",
    "\n",
    "The trainer class implements a unified interface for using the datagenerator.\n",
    "\n",
    "It supports fully connected or 1D convolutional networks, as well as other hyperparameters for the model and the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "NYw6y0peZ-ch"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__(self, *,\n",
    "               train_data,\n",
    "               val_data,\n",
    "               test_data,\n",
    "               feature_dim, \n",
    "               batch_size, \n",
    "               num_classes,\n",
    "               stacking_data = None,\n",
    "               augment=False,\n",
    "               class_to_number=None,\n",
    "               past_frames=0, \n",
    "               future_frames=0,\n",
    "               frame_gap=1, \n",
    "               use_conv=False):\n",
    "    flat_dim = np.prod(feature_dim)\n",
    "    if use_conv:\n",
    "      input_dim = ((past_frames + future_frames + 1), flat_dim,)\n",
    "    else:\n",
    "      input_dim = (flat_dim * (past_frames + future_frames + 1),)\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.use_conv=use_conv\n",
    "    self.num_classes=num_classes\n",
    "\n",
    "    c2n = {'other': 0,'investigation': 1,\n",
    "                'attack' : 2, 'mount' : 3}\n",
    "    self.class_to_number = class_to_number or c2n\n",
    "\n",
    "    self.train_generator = MABe_Generator(train_data, \n",
    "                                      batch_size=batch_size, \n",
    "                                      dim=input_dim,\n",
    "                                      num_classes=num_classes, \n",
    "                                      past_frames=past_frames, \n",
    "                                      future_frames=future_frames,\n",
    "                                      class_to_number=self.class_to_number,\n",
    "                                      use_conv=use_conv,\n",
    "                                      frame_gap=frame_gap,\n",
    "                                      augment=augment,\n",
    "                                      shuffle=True,\n",
    "                                      mode='fit')\n",
    "\n",
    "    self.val_generator = MABe_Generator(val_data, \n",
    "                                        batch_size=batch_size, \n",
    "                                        dim=input_dim, \n",
    "                                        num_classes=num_classes, \n",
    "                                        past_frames=past_frames,\n",
    "                                        future_frames=future_frames,\n",
    "                                        use_conv=use_conv,\n",
    "                                        class_to_number=self.class_to_number,\n",
    "                                        frame_gap=frame_gap,\n",
    "                                        augment=False,\n",
    "                                        shuffle=False,\n",
    "                                        mode='fit')\n",
    "    \n",
    "    if stacking_data is not None:\n",
    "        self.stacking_train_generator = MABe_Generator(stacking_data, \n",
    "                                            batch_size=batch_size, \n",
    "                                            dim=input_dim, \n",
    "                                            num_classes=num_classes, \n",
    "                                            past_frames=past_frames,\n",
    "                                            future_frames=future_frames,\n",
    "                                            use_conv=use_conv,\n",
    "                                            class_to_number=self.class_to_number,\n",
    "                                            frame_gap=frame_gap,\n",
    "                                            augment=False,\n",
    "                                            shuffle=False,\n",
    "                                            mode='fit')        \n",
    "    \n",
    "    if test_data is not None:\n",
    "      self.test_generator = MABe_Generator(test_data, \n",
    "                                        batch_size=8192, \n",
    "                                        dim=input_dim, \n",
    "                                        num_classes=num_classes, \n",
    "                                        past_frames=past_frames,\n",
    "                                        future_frames=future_frames,\n",
    "                                        use_conv=use_conv,\n",
    "                                        class_to_number=self.class_to_number,\n",
    "                                        frame_gap=frame_gap,\n",
    "                                        augment=False,\n",
    "                                        shuffle=False,\n",
    "                                        mode='predict')\n",
    "  \n",
    "  def delete_model(self):\n",
    "    self.model = None\n",
    "  \n",
    "  def initialize_model(self, layer_channels=(512, 256), dropout_rate=0., \n",
    "                       learning_rate=1e-3, conv_size=5):\n",
    "\n",
    "    def add_dense_bn_activate(model, out_dim, activation='relu', drop=0.):\n",
    "      model.add(layers.Dense(out_dim))\n",
    "      model.add(layers.BatchNormalization())\n",
    "      model.add(layers.Activation('relu'))\n",
    "      if drop > 0:\n",
    "        model.add(layers.Dropout(rate=drop))\n",
    "      return model\n",
    "    \n",
    "    def add_conv_bn_activate(model, out_dim, activation='relu', conv_size=3, drop=0.):\n",
    "      model.add(layers.Conv1D(out_dim, conv_size))\n",
    "      model.add(layers.BatchNormalization())\n",
    "      model.add(layers.Activation('relu'))\n",
    "      model.add(layers.MaxPooling1D(2, 2))\n",
    "      if drop > 0:\n",
    "        model.add(layers.Dropout(rate=drop))\n",
    "      return model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(self.input_dim))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    for ch in layer_channels:\n",
    "      if self.use_conv:\n",
    "        model = add_conv_bn_activate(model, ch, conv_size=conv_size,\n",
    "                                     drop=dropout_rate)\n",
    "      else:\n",
    "        model = add_dense_bn_activate(model, ch, drop=dropout_rate)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "    metrics = [tfa.metrics.F1Score(num_classes=self.num_classes)]\n",
    "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=metrics)\n",
    "\n",
    "    self.model = model\n",
    "\n",
    "  def _set_model(self, model):\n",
    "      \"\"\" Set an external, provide initialized and compiled keras model \"\"\"\n",
    "      self.model = model\n",
    "\n",
    "  def train(self, epochs=20, class_weight=None):\n",
    "    if self.model is None:\n",
    "      print(\"Please Call trainer.initialize_model first\")\n",
    "      return\n",
    "    self.model.fit(self.train_generator,\n",
    "          validation_data=self.val_generator,\n",
    "          epochs=epochs,\n",
    "          class_weight=class_weight)\n",
    "        \n",
    "  def get_validation_labels(self, on_test_set=False):\n",
    "    y_val = []\n",
    "    for _, y in self.val_generator:\n",
    "      y_val.extend(list(y))\n",
    "    y_val = np.argmax(np.array(y_val), axis=-1)\n",
    "    return y_val\n",
    "\n",
    "  def get_training_pred_proba(self):\n",
    "    y_val_pred = self.model.predict(self.train_generator)\n",
    "    return y_val_pred\n",
    "\n",
    "  def get_validation_pred_proba(self):\n",
    "    y_val_pred = self.model.predict(self.val_generator)\n",
    "    return y_val_pred\n",
    "\n",
    "  def get_validation_predictions(self):\n",
    "    y_val_pred = self.model.predict(self.val_generator)\n",
    "    y_val_pred = np.argmax(y_val_pred, axis=-1)\n",
    "    return y_val_pred\n",
    "\n",
    "  def get_stacking_trainer_pred_proba(self):\n",
    "    y_val_pred = self.model.predict(self.stacking_train_generator)\n",
    "    return y_val_pred\n",
    "\n",
    "  def get_trainer_predictions(self):\n",
    "    y_pred = self.model.predict(self.train_generator)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "    return y_pred\n",
    "\n",
    "  def get_stacking_trainer_predictions(self):\n",
    "    y_val_pred = self.model.predict(self.stacking_train_generator)\n",
    "    y_val_pred = np.argmax(y_val_pred, axis=-1)\n",
    "    return y_val_pred\n",
    "\n",
    "  def get_validation_metrics(self):\n",
    "    y_val = self.get_validation_labels()\n",
    "    y_val_pred = self.get_validation_predictions()\n",
    "\n",
    "    f1_scores = sklearn.metrics.f1_score(y_val, y_val_pred,average=None)\n",
    "    rec_scores = sklearn.metrics.precision_score(y_val, y_val_pred,average=None)\n",
    "    prec_scores = sklearn.metrics.recall_score(y_val, y_val_pred,average=None)\n",
    "    classes = list(self.class_to_number.keys())\n",
    "    metrics = pd.DataFrame({\"Class\": classes, \"F1\": f1_scores, \"Precision\": prec_scores, \"Recall\": rec_scores})\n",
    "    return metrics\n",
    "\n",
    "  def get_test_pred_proba(self):\n",
    "    all_test_preds = {}\n",
    "    for vkey in self.test_generator.video_keys:\n",
    "      nframes = self.test_generator.seq_lengths[vkey]\n",
    "      all_test_preds[vkey] = np.zeros((nframes, 4))\n",
    "\n",
    "    for X, vkey_fi_list in tqdm.tqdm(self.test_generator):\n",
    "      test_pred = np.array(self.model.predict(X))\n",
    "      #test_pred = np.argmax(test_pred, axis=-1)\n",
    "\n",
    "      for p, (vkey, fi) in zip(test_pred, vkey_fi_list):\n",
    "        all_test_preds[vkey][fi] = p\n",
    "    return all_test_preds\n",
    "\n",
    "  def get_test_predictions(self):\n",
    "    all_test_preds = {}\n",
    "    for vkey in self.test_generator.video_keys:\n",
    "      nframes = self.test_generator.seq_lengths[vkey]\n",
    "      all_test_preds[vkey] = np.zeros(nframes, dtype=np.int32)\n",
    "\n",
    "    for X, vkey_fi_list in tqdm.tqdm(self.test_generator):\n",
    "      test_pred = self.model.predict(X)\n",
    "      test_pred = np.argmax(test_pred, axis=-1)\n",
    "\n",
    "      for p, (vkey, fi) in zip(test_pred, vkey_fi_list):\n",
    "        all_test_preds[vkey][fi] = p\n",
    "    return all_test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYYrvXNTaVfU"
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "We'll normalize the data based on the information that the frame size is 1024x570\n",
    "\n",
    "The original data is of shape (sequence length, mouse, x y coordinate, keypoint)\n",
    " = (length, 2, 2, 7)\n",
    "\n",
    " We'll swap the x y and the keypoint axis, which will help in rotation augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "xYKbWFVcaSeW"
   },
   "outputs": [],
   "source": [
    "def normalize_data(orig_pose_dictionary):\n",
    "  for key in orig_pose_dictionary:\n",
    "    X = orig_pose_dictionary[key]['keypoints']\n",
    "    X = X.transpose((0,1,3,2)) #last axis is x, y coordinates\n",
    "    X[..., 0] = X[..., 0]/1024\n",
    "    X[..., 1] = X[..., 1]/570\n",
    "    orig_pose_dictionary[key]['keypoints'] = X\n",
    "  return orig_pose_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKOjPhZRL4gk"
   },
   "source": [
    "## Dataset split\n",
    "Since MABe has multiple sequences, it is sensible to split the dataset based on different sequences rather than randomly sampling frames, which may leak information.\n",
    "\n",
    "About half the sequences don't have \"attack\" behavior, hence we'll stratify based on whether \"attack\" behavior is present or absent.\n",
    "\n",
    "This function only does a single split, but you can also do multiple splits for cross validation.\n",
    "\n",
    "For Task 2 and 3 there are very few sequences, hence we split the sequences in half for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "C7iuMRKKakls"
   },
   "outputs": [],
   "source": [
    "def split_validation(orig_pose_dictionary, vocabulary, seed=2021, \n",
    "                       test_size=0.5, split_videos=False):\n",
    "  if split_videos:\n",
    "    pose_dictionary = {}\n",
    "    for key in orig_pose_dictionary:\n",
    "      key_pt1 = key + '_part1'\n",
    "      key_pt2 = key + '_part2'\n",
    "      anno_len = len(orig_pose_dictionary[key]['annotations'])\n",
    "      split_idx = anno_len//2\n",
    "      pose_dictionary[key_pt1] = {\n",
    "          'annotations': orig_pose_dictionary[key]['annotations'][:split_idx],\n",
    "          'keypoints': orig_pose_dictionary[key]['keypoints'][:split_idx]}\n",
    "      pose_dictionary[key_pt2] = {\n",
    "          'annotations': orig_pose_dictionary[key]['annotations'][split_idx:],\n",
    "          'keypoints': orig_pose_dictionary[key]['keypoints'][split_idx:]}\n",
    "  else:\n",
    "    pose_dictionary = orig_pose_dictionary\n",
    "  \n",
    "  def get_percentage(sequence_key):\n",
    "    anno_seq = num_to_text(pose_dictionary[sequence_key]['annotations'])\n",
    "    counts = {k: np.mean(np.array(anno_seq) == k) for k in vocabulary}\n",
    "    return counts\n",
    "\n",
    "  anno_percentages = {k: get_percentage(k) for k in pose_dictionary}\n",
    "\n",
    "  anno_perc_df = pd.DataFrame(anno_percentages).T\n",
    "\n",
    "  rng_state = np.random.RandomState(seed)\n",
    "  try:\n",
    "    idx_train, idx_val = train_test_split(anno_perc_df.index,\n",
    "                                      stratify=anno_perc_df['attack'] > 0, \n",
    "                                      test_size=test_size,\n",
    "                                      random_state=rng_state)\n",
    "  except:\n",
    "    idx_train, idx_val = train_test_split(anno_perc_df.index,\n",
    "                                      test_size=test_size,\n",
    "                                      random_state=rng_state)\n",
    "    \n",
    "  train_data = {k : pose_dictionary[k] for k in idx_train}\n",
    "  val_data = {k : pose_dictionary[k] for k in idx_val}\n",
    "  return train_data, val_data, anno_perc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This version makes two validation datasets... one for fitting the stacked estimator\n",
    "#and another for computing test performance\n",
    "\n",
    "#Removes support for split videos\n",
    "def split_validation_two(orig_pose_dictionary, vocabulary, seed=2021, \n",
    "                       test_size = 0.2, split_videos=False, stacking_size = 0.2):\n",
    "    \n",
    "  total_size = min(test_size + stacking_size, 0.5)\n",
    "\n",
    "  pose_dictionary = orig_pose_dictionary\n",
    "  \n",
    "  def get_percentage(sequence_key):\n",
    "    anno_seq = num_to_text(pose_dictionary[sequence_key]['annotations'])\n",
    "    counts = {k: np.mean(np.array(anno_seq) == k) for k in vocabulary}\n",
    "    return counts\n",
    "\n",
    "  anno_percentages = {k: get_percentage(k) for k in pose_dictionary}\n",
    "\n",
    "  anno_perc_df = pd.DataFrame(anno_percentages).T\n",
    "\n",
    "  rng_state = np.random.RandomState(seed)\n",
    "  try:\n",
    "    idx_train, idx = train_test_split(anno_perc_df.index,\n",
    "                                      stratify=anno_perc_df['attack'] > 0, \n",
    "                                      test_size=total_size,\n",
    "                                      random_state=rng_state)\n",
    "    idx_test, idx_val = train_test_split(idx,\n",
    "                                  test_size=stacking_size/total_size,\n",
    "                                  random_state=rng_state)\n",
    "    print(\"Successfully stratified training data\")    \n",
    "  except:\n",
    "    idx_train, idx = train_test_split(anno_perc_df.index,\n",
    "                                      test_size=total_size,\n",
    "                                      random_state=rng_state)\n",
    "    idx_test, idx_val = train_test_split(idx,\n",
    "                                  test_size=stacking_size/total_size,\n",
    "                                  random_state=rng_state)\n",
    "    \n",
    "  train_data = {k : pose_dictionary[k] for k in idx_train}\n",
    "  val_data = {k : pose_dictionary[k] for k in idx_val}\n",
    "  test_data = {k : pose_dictionary[k] for k in idx_test}\n",
    "  return train_data, val_data, test_data, anno_perc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neIzSJVvMwyo"
   },
   "source": [
    "# Train function and inference\n",
    "\n",
    "This below function is specific for Task 1, it has a set of hyperparameters we found with some tuning. Though results can be improved with further tuning.\n",
    "\n",
    "It also generates the submission dictionary after training is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get baseline trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "results_dir = './results_baseline_net/'\n",
    "dataset = train['sequences']\n",
    "vocabulary = train['vocabulary']\n",
    "test_data = test['sequences']\n",
    "seed = 2021\n",
    "epochs = 50\n",
    "\n",
    "augment = False\n",
    "skip_test_prediction = False\n",
    "\n",
    "stacking_size = 0.2\n",
    "\n",
    "HPARAMS = {}\n",
    "val_size = HPARAMS[\"val_size\"] = 0.2\n",
    "normalize = HPARAMS[\"normalize\"] = True\n",
    "HPARAMS[\"seed\"] = seed\n",
    "seed_everything(seed)\n",
    "split_videos = HPARAMS[\"split_videos\"] = False\n",
    "\n",
    "# Generator parameters\n",
    "past_frames = HPARAMS[\"past_frames\"] = 50\n",
    "future_frames = HPARAMS[\"future_frames\"] = 50\n",
    "frame_gap = HPARAMS[\"frame_gap\"] = 1\n",
    "use_conv = HPARAMS[\"use_conv\"] = True\n",
    "batch_size = HPARAMS[\"batch_size\"] = 128\n",
    "\n",
    "# Model parameters\n",
    "dropout_rate = HPARAMS[\"dropout_rate\"] = 0.5\n",
    "learning_rate = HPARAMS[\"learning_rate\"] = 5e-4\n",
    "layer_channels = HPARAMS[\"layer_channels\"] = (128, 64, 32)\n",
    "conv_size = HPARAMS[\"conv_size\"] = 5\n",
    "augment = HPARAMS[\"augment\"] = augment\n",
    "class_to_number = HPARAMS['class_to_number'] = vocabulary\n",
    "epochs = HPARAMS[\"epochs\"] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stratified training data\n"
     ]
    }
   ],
   "source": [
    "if normalize:\n",
    "    dataset = normalize_data(deepcopy(dataset))\n",
    "if not skip_test_prediction:\n",
    "    test_data = normalize_data(deepcopy(test_data))\n",
    "else:\n",
    "    test_data = None\n",
    "\n",
    "train_data, val_data, stacking_training_data, anno_perc_df = split_validation_two(dataset, \n",
    "                                                    seed=seed,\n",
    "                                                    vocabulary=vocabulary,\n",
    "                                                    test_size=val_size, \n",
    "                                                    stacking_size = stacking_size,\n",
    "                                                    split_videos=split_videos)                               \n",
    "num_classes = len(anno_perc_df.keys())\n",
    "feature_dim = HPARAMS[\"feature_dim\"] = (2,7,2)\n",
    "\n",
    "trainer = Trainer(train_data=train_data,\n",
    "                val_data=val_data,\n",
    "                test_data=test_data,\n",
    "                stacking_data = stacking_training_data,\n",
    "                feature_dim=feature_dim, \n",
    "                batch_size=batch_size, \n",
    "                num_classes=num_classes,\n",
    "                augment=augment,\n",
    "                class_to_number=class_to_number,\n",
    "                past_frames=past_frames, \n",
    "                future_frames=future_frames,\n",
    "                frame_gap=frame_gap,\n",
    "                use_conv=use_conv)\n",
    "\n",
    "trainer.initialize_model(layer_channels=layer_channels,\n",
    "                      dropout_rate=dropout_rate,\n",
    "                      learning_rate=learning_rate,\n",
    "                      conv_size=conv_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2579/2579 [==============================] - 21s 8ms/step - loss: 0.6161 - f1_score: 0.4776 - val_loss: 0.4088 - val_f1_score: 0.5593\n",
      "Epoch 2/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.3409 - f1_score: 0.7624 - val_loss: 0.3493 - val_f1_score: 0.6816\n",
      "Epoch 3/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.3024 - f1_score: 0.8118 - val_loss: 0.3618 - val_f1_score: 0.6824\n",
      "Epoch 4/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2900 - f1_score: 0.8248 - val_loss: 0.3639 - val_f1_score: 0.6855\n",
      "Epoch 5/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2726 - f1_score: 0.8346 - val_loss: 0.3561 - val_f1_score: 0.6671\n",
      "Epoch 6/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2670 - f1_score: 0.8415 - val_loss: 0.3406 - val_f1_score: 0.7140\n",
      "Epoch 7/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2557 - f1_score: 0.8488 - val_loss: 0.3437 - val_f1_score: 0.7081\n",
      "Epoch 8/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2497 - f1_score: 0.8523 - val_loss: 0.3520 - val_f1_score: 0.6963\n",
      "Epoch 9/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2425 - f1_score: 0.8595 - val_loss: 0.3609 - val_f1_score: 0.6967\n",
      "Epoch 10/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2389 - f1_score: 0.8626 - val_loss: 0.3463 - val_f1_score: 0.7100\n",
      "Epoch 11/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2345 - f1_score: 0.8645 - val_loss: 0.3667 - val_f1_score: 0.6914\n",
      "Epoch 12/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2305 - f1_score: 0.8674 - val_loss: 0.3609 - val_f1_score: 0.6998\n",
      "Epoch 13/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2236 - f1_score: 0.8716 - val_loss: 0.3478 - val_f1_score: 0.7099\n",
      "Epoch 14/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2219 - f1_score: 0.8712 - val_loss: 0.3708 - val_f1_score: 0.6894\n",
      "Epoch 15/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2194 - f1_score: 0.8742 - val_loss: 0.3679 - val_f1_score: 0.6949\n",
      "Epoch 16/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2159 - f1_score: 0.8772 - val_loss: 0.3691 - val_f1_score: 0.6873\n",
      "Epoch 17/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2111 - f1_score: 0.8788 - val_loss: 0.3643 - val_f1_score: 0.6998\n",
      "Epoch 18/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2119 - f1_score: 0.8794 - val_loss: 0.3880 - val_f1_score: 0.6974\n",
      "Epoch 19/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2086 - f1_score: 0.8825 - val_loss: 0.3688 - val_f1_score: 0.7024\n",
      "Epoch 20/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2050 - f1_score: 0.8836 - val_loss: 0.3852 - val_f1_score: 0.6820\n",
      "Epoch 21/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2019 - f1_score: 0.8852 - val_loss: 0.3950 - val_f1_score: 0.6985\n",
      "Epoch 22/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.2021 - f1_score: 0.8831 - val_loss: 0.3649 - val_f1_score: 0.7055\n",
      "Epoch 23/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1995 - f1_score: 0.8880 - val_loss: 0.3908 - val_f1_score: 0.6971\n",
      "Epoch 24/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1953 - f1_score: 0.8913 - val_loss: 0.3827 - val_f1_score: 0.7043\n",
      "Epoch 25/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1939 - f1_score: 0.8897 - val_loss: 0.3920 - val_f1_score: 0.7019\n",
      "Epoch 26/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1957 - f1_score: 0.8904 - val_loss: 0.3774 - val_f1_score: 0.6958\n",
      "Epoch 27/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1925 - f1_score: 0.8933 - val_loss: 0.3792 - val_f1_score: 0.6994\n",
      "Epoch 28/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1925 - f1_score: 0.8913 - val_loss: 0.3859 - val_f1_score: 0.6945\n",
      "Epoch 29/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1905 - f1_score: 0.8921 - val_loss: 0.3920 - val_f1_score: 0.7040\n",
      "Epoch 30/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1886 - f1_score: 0.8943 - val_loss: 0.3826 - val_f1_score: 0.6994\n",
      "Epoch 31/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1876 - f1_score: 0.8950 - val_loss: 0.3840 - val_f1_score: 0.7002\n",
      "Epoch 32/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1859 - f1_score: 0.8949 - val_loss: 0.3904 - val_f1_score: 0.7158\n",
      "Epoch 33/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1860 - f1_score: 0.8955 - val_loss: 0.3874 - val_f1_score: 0.7054\n",
      "Epoch 34/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1848 - f1_score: 0.8962 - val_loss: 0.3942 - val_f1_score: 0.6967\n",
      "Epoch 35/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1833 - f1_score: 0.8973 - val_loss: 0.3604 - val_f1_score: 0.7038\n",
      "Epoch 36/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1815 - f1_score: 0.8983 - val_loss: 0.3895 - val_f1_score: 0.7015\n",
      "Epoch 37/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1832 - f1_score: 0.8986 - val_loss: 0.3894 - val_f1_score: 0.7052\n",
      "Epoch 38/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1810 - f1_score: 0.8982 - val_loss: 0.3906 - val_f1_score: 0.7104\n",
      "Epoch 39/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1801 - f1_score: 0.8997 - val_loss: 0.4004 - val_f1_score: 0.7055\n",
      "Epoch 40/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1775 - f1_score: 0.8997 - val_loss: 0.3857 - val_f1_score: 0.7011\n",
      "Epoch 41/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1792 - f1_score: 0.8998 - val_loss: 0.3901 - val_f1_score: 0.7170\n",
      "Epoch 42/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1744 - f1_score: 0.9017 - val_loss: 0.3996 - val_f1_score: 0.6945\n",
      "Epoch 43/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1778 - f1_score: 0.9017 - val_loss: 0.3931 - val_f1_score: 0.7220\n",
      "Epoch 44/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1770 - f1_score: 0.9010 - val_loss: 0.3811 - val_f1_score: 0.7111\n",
      "Epoch 45/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1748 - f1_score: 0.9019 - val_loss: 0.3982 - val_f1_score: 0.7157\n",
      "Epoch 46/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1750 - f1_score: 0.9022 - val_loss: 0.3938 - val_f1_score: 0.7100\n",
      "Epoch 47/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1751 - f1_score: 0.9008 - val_loss: 0.3973 - val_f1_score: 0.7132\n",
      "Epoch 48/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1743 - f1_score: 0.9037 - val_loss: 0.3861 - val_f1_score: 0.7127\n",
      "Epoch 49/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1730 - f1_score: 0.9038 - val_loss: 0.4047 - val_f1_score: 0.7196\n",
      "Epoch 50/50\n",
      "2579/2579 [==============================] - 20s 8ms/step - loss: 0.1732 - f1_score: 0.9037 - val_loss: 0.4308 - val_f1_score: 0.7006\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epochs=epochs)\n",
    "augment_str = '_augmented' if augment else ''\n",
    "trainer.model.save(f'{results_dir}/task1{augment_str}.h5')\n",
    "np.save(f\"{results_dir}/task1{augment_str}_hparams\", HPARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predicted probabilities for input into stacked model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_proba = trainer.get_validation_pred_proba()\n",
    "y_train_pred_proba = trainer.get_training_pred_proba()\n",
    "y_stacking_train_pred_proba = trainer.get_stacking_trainer_pred_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = trainer.get_validation_predictions()\n",
    "#y_train_pred = trainer.get_trainer_predictions()\n",
    "y_stacking_train_pred = trainer.get_stacking_trainer_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_matrices(generator):\n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    for x,y in generator:\n",
    "        x_all.extend(list(x))\n",
    "        y_all.extend(list(y))\n",
    "\n",
    "    x_all = np.array(x_all)\n",
    "    y_all = np.array(y_all)        \n",
    "    return x_all, y_all\n",
    "\n",
    "#Make the feature space for this thing.....\n",
    "def make_features(X):\n",
    "    s = X.shape\n",
    "    #              fr    of    mo bp xy\n",
    "    t = X.reshape((s[0], s[1], 2, 7, 2))\n",
    "    features = np.zeros((s[0], s[1], 7*7 + 2*21))\n",
    "    count = 0\n",
    "    for i in range(7):\n",
    "        print(f\"processing body part {i}\")\n",
    "        for j in range(7):\n",
    "            if i < j:\n",
    "                #Intra-distance\n",
    "                for m in range(2):\n",
    "                    intra_dist = np.sqrt((t[:,:,m,i,0] - t[:,:,m,j,0])**2 + (t[:,:,m,i,1] - t[:,:,m,j,1])**2)\n",
    "                    features[:,:,count] = intra_dist\n",
    "                    count += 1\n",
    "            #Inter distance\n",
    "            inter_dist = np.sqrt((t[:,:,0,i,0] - t[:,:,1,j,0])**2 + (t[:,:,0,i,1] - t[:,:,1,j,1])**2)\n",
    "            features[:,:,count] = inter_dist\n",
    "            count += 1\n",
    "            \n",
    "    return features\n",
    "\n",
    "#Get rid of some of the extra features\n",
    "def skip_time_lags(x, skip = 20):\n",
    "    x_ = x.copy()\n",
    "    indices = list(range(0, 120, skip))\n",
    "    x_ = x_[:, indices, :]\n",
    "    return x_\n",
    "\n",
    "def compute_differences_time(X):\n",
    "    periods = [-20, -10, 10, 20]\n",
    "    data = [X.diff(p) for p in periods] + [X]\n",
    "\n",
    "def make_data(generator):\n",
    "    X, y = get_data_matrices(generator)\n",
    "    X = make_features(X)\n",
    "    X = skip_time_lags(X)\n",
    "    X = X.reshape((X.shape[0], -1))\n",
    "    y = np.argmax(y, axis=-1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing body part 0\n",
      "processing body part 1\n",
      "processing body part 2\n",
      "processing body part 3\n",
      "processing body part 4\n",
      "processing body part 5\n",
      "processing body part 6\n",
      "processing body part 0\n",
      "processing body part 1\n",
      "processing body part 2\n",
      "processing body part 3\n",
      "processing body part 4\n",
      "processing body part 5\n",
      "processing body part 6\n",
      "processing body part 0\n",
      "processing body part 1\n",
      "processing body part 2\n",
      "processing body part 3\n",
      "processing body part 4\n",
      "processing body part 5\n",
      "processing body part 6\n"
     ]
    }
   ],
   "source": [
    "X_st, y_st_train = make_data(trainer.stacking_train_generator)\n",
    "X_tr, y_train = make_data(trainer.train_generator)\n",
    "X_va, y_val = make_data(trainer.val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting models:\n",
      "- rf\n",
      "- lr\n"
     ]
    }
   ],
   "source": [
    "#Make random forest classifier, with group-level CV\n",
    "models = {'rf': RandomForestClassifier(n_estimators = 10),\n",
    "         'lr': sklearn.linear_model.LogisticRegression(max_iter = 10000)}\n",
    "\n",
    "print('Fitting models:')\n",
    "for m_name in models:\n",
    "    print(\"-\", m_name)\n",
    "    models[m_name].fit(X_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_features_va = y_val_pred_proba\n",
    "stacked_features_st = y_stacking_train_pred_proba\n",
    "stacked_features_tr = y_train_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330112, 546)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on validation data and test data\n",
    "pred_train = {}\n",
    "pred_val = {}\n",
    "pred_st = {}\n",
    "\n",
    "pred_train_proba = {}\n",
    "pred_st_proba = {}\n",
    "pred_val_proba = {}\n",
    "\n",
    "for m in models:\n",
    "    model = models[m]\n",
    "\n",
    "    #Compute performance measures\n",
    "    pred_train[m] = model.predict(X_tr)\n",
    "    pred_val[m] = model.predict(X_va)\n",
    "    pred_st[m] = model.predict(X_st)\n",
    "\n",
    "    pred_train_proba[m] = model.predict_proba(X_tr)\n",
    "    pred_st_proba[m] = model.predict_proba(X_st)\n",
    "    pred_val_proba[m] = model.predict_proba(X_va)\n",
    "    \n",
    "    stacked_features_va = np.hstack((stacked_features_va, pred_val_proba[m]))\n",
    "    stacked_features_st = np.hstack((stacked_features_st, pred_st_proba[m]))\n",
    "    stacked_features_tr = np.hstack((stacked_features_tr, pred_train_proba[m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at correlation between predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.83439257, 0.84502799],\n",
       "       [0.83439257, 1.        , 0.84974471],\n",
       "       [0.84502799, 0.84974471, 1.        ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.array(list(pred_val.values()))\n",
    "preds = np.vstack((preds, y_val_pred))\n",
    "np.corrcoef(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train logistic regression (from training data)\n",
    "final_model = LogisticRegression(max_iter = 10000)\n",
    "\n",
    "print('Fitting final model')\n",
    "final_model.fit(stacked_features_st, y_st_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "y_stacked_st = final_model.predict(stacked_features_st)\n",
    "y_stacked_va = final_model.predict(stacked_features_va)\n",
    "y_stacked_tr = final_model.predict(stacked_features_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y, pred):\n",
    "    re = recall_score(y, pred, average = 'macro', labels = [0,1,2])\n",
    "    pr = precision_score(y, pred, average = 'macro', labels = [0,1,2])\n",
    "    f1 = f1_score(y, pred, labels = [0,1,2], average = 'macro')\n",
    "    return [re, pr, f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5926282465663969, recall: 0.6675779313384268, precision: 0.613398152670745\n"
     ]
    }
   ],
   "source": [
    "#Validation data (not used for training underlying model, or stacked ensembl)\n",
    "print_metrics(compute_metrics(y_val, y_stacked_va))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6925820704967526, recall: 0.7773195159035139, precision: 0.7193597268770299\n"
     ]
    }
   ],
   "source": [
    "#Performance on stacked ensemble training data\n",
    "print_metrics(compute_metrics(y_st_train, y_stacked_st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.29611383459887713, recall: 0.5146810554645531, precision: 0.3370909906467329\n"
     ]
    }
   ],
   "source": [
    "#Performance on base model training data\n",
    "print_metrics(compute_metrics(y_train, y_stacked_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.4594318065089649, recall: 0.5381360495677029, precision: 0.4695136807169081\n"
     ]
    }
   ],
   "source": [
    "print_metrics(compute_metrics(y_val, pred_val['rf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9975561592936438, recall: 0.9991434709616888, precision: 0.9983473245793335\n"
     ]
    }
   ],
   "source": [
    "print_metrics(compute_metrics(y_train, pred_train['rf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.584133041982892, recall: 0.6059187085329404, precision: 0.5893656806034983\n"
     ]
    }
   ],
   "source": [
    "print_metrics(compute_metrics(y_st_train, pred_st['rf']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6223424475585833, recall: 0.6684867596984881, precision: 0.6249145701618243\n"
     ]
    }
   ],
   "source": [
    "print_metrics(compute_metrics(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6652044833526417, recall: 0.7123358575439322, precision: 0.660325917692368\n"
     ]
    }
   ],
   "source": [
    "print_metrics(compute_metrics(y_st_train, y_stacking_train_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.1448585742051959, recall: 0.12845538148627347, precision: 0.13602431226185224\n"
     ]
    }
   ],
   "source": [
    "print_metrics(compute_metrics(y_train, np.argmax(y_train_pred_proba, axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trainer prep for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting validation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [16:24<00:00,  1.01it/s]\n",
      "100%|██████████| 998/998 [11:21<00:00,  1.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3388"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val_predictions = trainer.get_validation_predictions()\n",
    "print(\"Getting validation metrics\")\n",
    "val_metrics = trainer.get_validation_metrics()\n",
    "val_metrics.to_csv(f\"{results_dir}/task1_metrics_val.csv\", index=False)\n",
    "\n",
    "if not skip_test_prediction:\n",
    "    test_results = trainer.get_test_predictions()\n",
    "    test_proba = trainer.get_test_pred_proba()\n",
    "    np.save(f\"{results_dir}/test_results\", test_results)\n",
    "    np.save(f\"{results_dir}/test_results_proba\", test_proba)\n",
    "else:\n",
    "    test_results = {}\n",
    "    test_proba = {}\n",
    "\n",
    "#del trainer # clear ram as the test dataset is large\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [11:46<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "self = trainer\n",
    "all_test_preds = {}\n",
    "for vkey in self.test_generator.video_keys:\n",
    "  nframes = self.test_generator.seq_lengths[vkey]\n",
    "  all_test_preds[vkey] = np.zeros((nframes, 4))\n",
    "\n",
    "for X, vkey_fi_list in tqdm.tqdm(self.test_generator):\n",
    "    test_pred = self.model.predict(X)\n",
    "    #test_pred = np.argmax(test_pred, axis=-1)\n",
    "\n",
    "    for p, (vkey, fi) in zip(test_pred, vkey_fi_list):\n",
    "        all_test_preds[vkey][fi, :] = p\n",
    "#return all_test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPfKXGb1vTRd"
   },
   "source": [
    "## Validate and save the prediction as `npy` 📨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_submission(submission, sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1LkuPd5AvTRd"
   },
   "outputs": [],
   "source": [
    "np.save(\"submission.npy\", submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEZqmoHJJl4j"
   },
   "source": [
    "## Submit to AIcrowd 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaUu2j8tCdZ1",
    "outputId": "ff0d3f06-544b-443b-e311-d91e370822f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[1;34msubmission.npy\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100.0%\u001b[0m • \u001b[32m32.7/32.7 MB\u001b[0m • \u001b[31m3.3 MB/s\u001b[0m • \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h                                                                    ╭─────────────────────────╮                                                                    \n",
      "                                                                    │ \u001b[1mSuccessfully submitted!\u001b[0m │                                                                    \n",
      "                                                                    ╰─────────────────────────╯                                                                    \n",
      "\u001b[3m                                                                          Important links                                                                          \u001b[0m\n",
      "┌──────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│  This submission │\u001b[1;94m \u001b[0m\u001b[1;94mhttps://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications/submissions/125873             \u001b[0m\u001b[1;94m \u001b[0m│\n",
      "│                  │                                                                                                                                              │\n",
      "│  All submissions │\u001b[1;94m \u001b[0m\u001b[1;94mhttps://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications/submissions?my_submissions=true\u001b[0m\u001b[1;94m \u001b[0m│\n",
      "│                  │                                                                                                                                              │\n",
      "│      Leaderboard │\u001b[1;94m \u001b[0m\u001b[1;94mhttps://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications/leaderboards                   \u001b[0m\u001b[1;94m \u001b[0m│\n",
      "│                  │                                                                                                                                              │\n",
      "│ Discussion forum │\u001b[1;94m \u001b[0m\u001b[1;94mhttps://discourse.aicrowd.com/c/multi-agent-behavior-representation-modeling-measurement-and-applications                                   \u001b[0m\u001b[1;94m \u001b[0m│\n",
      "│                  │                                                                                                                                              │\n",
      "│   Challenge page │\u001b[1;94m \u001b[0m\u001b[1;94mhttps://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications                                \u001b[0m\u001b[1;94m \u001b[0m│\n",
      "└──────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "{'submission_id': 125873, 'created_at': '2021-03-11T20:11:38.937Z'}\n"
     ]
    }
   ],
   "source": [
    "!aicrowd submission create -c mabe-task-1-classical-classification -f submission.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVOhbQBK4CdL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of MABe Task 1: Classical Classification Baseline",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
